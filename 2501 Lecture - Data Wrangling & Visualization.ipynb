{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2501 Lecture - Data Wrangling / Visualization\n",
    "\n",
    "\n",
    "### Ryan Kazmerik\n",
    "* Data Scientist, Encana Corporation\n",
    "* Sessional Instructor, Mount Royal University\n",
    "* Mount Royal University, Bachelor CIS (2011)\n",
    "* Wilfrid Laurier University, Master MAC (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install altair\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with our first data representation: Row based files\n",
    "\n",
    "## For this we'll load up our data in CSV format, and use the built in Python library CSV to read the contents of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('articles.csv',  encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        print(\", \".join(row), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV is a great storage format, compact, and readable - but a little clumsy to work with.\n",
    "\n",
    "## Let's convert this CSV into our 2nd data structure: List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = []\n",
    "\n",
    "with open('articles.csv',  encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        articles_list.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how many articles are in our list (a.k.a length of the list), and how many columns of metadata each article has (a.k.a length of the first item):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_articles = #TODO: GET LENGTH OF THE LIST \n",
    "total_columns = #TODO: GET LENGTH OF FIRST ITEM IN LIST\n",
    "\n",
    "print('Total number of articles:', total_articles) #a.k.a the length of the list\n",
    "\n",
    "print('Total number of columns:', total_columns) #a.k.a the length of the first object in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A list wouldn't be a list if we couldn't iterate through the items.\n",
    "\n",
    "## Let's use Pythons built in range function [0:9] to print the titles of the first 10 articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles_list[0:10]: #not ideal that the first item is always the column name\n",
    "    \n",
    "    print(article[4], end=\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With a list, we can easily get some basic stats on the articles and iterate through the items.\n",
    "\n",
    "## But if we want to add a new property to each item, lists can be difficult to work with.. so it's best to convert our list items into our 3rd data structure : Dictionary\n",
    "\n",
    "## Let's convert the original CSV to a Dictionary this time, and print the first 10 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = [a for a in csv.DictReader(open('articles.csv', encoding=\"utf8\"))]\n",
    "\n",
    "for article in articles_dict[0:10]:\n",
    "\n",
    "    #QUESTION: WHY IS THIS METHOD OF REFERENCING THE COLUMN BETTER THAN THE LIST METHOD?\n",
    "    print(article['title'], end=\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's add our new property to each article (word count of article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles_dict:\n",
    "    \n",
    "    words = # TODO: create a list of words using the split() function\n",
    "    word_count = # TODO: get the length of the words list\n",
    "    \n",
    "    article['word_count'] = word_count\n",
    "    \n",
    "print(article[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dictionaries can accomplish this task, but compiling aggregations (groupings) is a bit tricky.\n",
    "\n",
    "## Let's convert our dataset into our 4th data structure : Data Frame \n",
    "\n",
    "## We can use the popular library Pandas to convert straight from our Dictionary to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "df = pd.DataFrame.from_dict(json_normalize(articles_dict), orient='columns')\n",
    "\n",
    "df.info() #displays some basic info on the dataframe\n",
    "df.head() #prints out the first 5 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can easily produce some aggregations like: top 10 sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sources = df.groupby(['source']).agg({ #QUESTION: WHY ARE WE COPYING INTO A NEW DATA FRAME?\n",
    "    'id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "df_sources.sort_values(by=['id'], inplace=True, ascending=False)\n",
    "\n",
    "df_sources.columns = ['source', 'num_articles']\n",
    "\n",
    "df_sources.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It might be nice to see this data visualized, and luckily a Data Frame transposes nicely into many Python viz libraries.\n",
    "\n",
    "## We'll use one called Altair to produce a vertical bar chart of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(df_sources.head(10)).mark_bar().encode(\n",
    "    x='num_articles',\n",
    "    y='source'\n",
    ").properties(\n",
    "    title='Number of Articles by Source',\n",
    "    width=750\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is interesting but doesn't really tell us much about what the articles are talking about.\n",
    "\n",
    "## Let's apply some data science using Natural Language Processing to help us to extract some more meaningful metadata from the articles.\n",
    "\n",
    "## We can use a popular NLP library called SpaCy to do the text processing:\n",
    "\n",
    "https://explosion.ai/demos/displacy-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "nlp = sp.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_locations(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    ents = [e.text for e in doc.ents if e.label_ == \"LOC\"]\n",
    "    \n",
    "    if len(ents) == 0:\n",
    "        ents = None\n",
    "    else:\n",
    "        ents = ents[0]\n",
    "        \n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['locations'] = df['description'].map(extract_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's aggregate and visualize our new locations column to show the top 10 locations mentioned in the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = df.groupby(['locations']).agg({\n",
    "    'id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "df_locations.sort_values(by=['id'], inplace=True, ascending=False)\n",
    "\n",
    "df_locations.columns = ['location', 'num_articles']\n",
    "\n",
    "df_sources.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df_locations.head(10)).mark_bar().encode(\n",
    "    x='num_articles',\n",
    "    y='location'\n",
    ").properties(\n",
    "    title='Number of Articles by Location',\n",
    "    width=750\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CREATE ANOTHER COLUMN IN OUR df DATAFRAME TO EXTRACT THE ORGANIZATIONS (ORG) FROM THE ARTICLES\n",
    "\n",
    "#TODO: CREATE A NEW DATAFRAME CALLED df_organizations AND GROUP BY THE ORGANIZATION\n",
    "\n",
    "#TODO: CREATE A NEW BAR CHART TO VISUALIZE THE ORGANIZATION DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
